{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Num GPUs Available:  0\n",
      "data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#%%\n",
    "\n",
    "folder = os.path.join('..','..','short_data_4_model','50')\n",
    "\n",
    "\n",
    "data_train = np.load(os.path.join(folder, 'train_data.npz'),'r')\n",
    "data_train = data_train['arr_0']\n",
    "\n",
    "\n",
    "\n",
    "data_valid = np.load(os.path.join(folder, 'valid_data.npz'),'r')\n",
    "data_valid = data_valid['arr_0']\n",
    "'''\n",
    "data_valid = np.load(os.path.join(folder, 'valid_data.npz'),'r')\n",
    "data_valid = data_valid['arr_0']\n",
    "\n",
    "data_test = np.load(os.path.join(folder, 'test_data.npz'),'r')\n",
    "data_test = data_test['arr_0']\n",
    "'''\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(183666, 50, 3)\n",
      "(52449, 50, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ndata_train = data_train[0:80000] \\ndata_valid = data_valid[0:30000]\\nprint(np.shape(data_train))\\nprint(np.shape(data_valid))\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.shape(data_train))\n",
    "print(np.shape(data_valid))\n",
    "'''\n",
    "data_train = data_train[0:80000] \n",
    "data_valid = data_valid[0:30000]\n",
    "print(np.shape(data_train))\n",
    "print(np.shape(data_valid))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "seq_len =50\n",
    "units = 200\n",
    "# no use\n",
    "embedding_dim = 256\n",
    "\n",
    "num_examples = len(data_train)\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_train).shuffle(num_examples)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Encoder output shape: (batch size, sequence length, units) (60, 50, 200)\n",
      "Encoder Hidden state shape: (batch size, units) (60, 200)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.lstm = keras.layers.LSTM(self.encoding_units,\n",
    "                                     return_sequences=True,\n",
    "                                     return_state=True,\n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, m,h):\n",
    "        output,m, h = self.lstm(x, initial_state = (m,h))\n",
    "        return output,m, h\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units)),tf.zeros((self.batch_size,self.encoding_units))\n",
    "\n",
    "\n",
    "example_input_batch = np.zeros((BATCH_SIZE,seq_len,3))\n",
    "# sample input\n",
    "encoder = Encoder( embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "\n",
    "m,h = encoder.initialize_hidden_state()\n",
    "sample_output, m,h = encoder(example_input_batch, m,h)\n",
    "\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(m.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (60, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "\n",
    "        self.lstm = keras.layers.LSTM(self.decoding_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x, m,h):\n",
    "\n",
    "        output, m,h = self.lstm(x,initial_state = (m,h))\n",
    "\n",
    "        #output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "        #output = tf.expand_dims(output,1)\n",
    "\n",
    "\n",
    "        return x, m,h\n",
    "\n",
    "vocab_size=3\n",
    "decoder = Decoder( vocab_size ,embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1,3)),\n",
    "                                      m,h)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50full_seq2seq_200_60\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError( reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, [0,0,0]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_dir = str(seq_len)+'full_seq2seq_'+str(units)+'_'+str(BATCH_SIZE)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "c = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "print(checkpoint_dir)\n",
    "#checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.0067\n",
      "Epoch 1 Batch 100 Loss 0.0087\n",
      "Epoch 1 Batch 200 Loss 0.0093\n",
      "Epoch 1 Batch 300 Loss 0.0090\n",
      "Epoch 1 Batch 400 Loss 0.0082\n",
      "Epoch 1 Batch 500 Loss 0.0065\n",
      "Epoch 1 Batch 600 Loss 0.0064\n",
      "Epoch 1 Batch 700 Loss 0.0070\n",
      "Epoch 1 Batch 800 Loss 0.0085\n",
      "Epoch 1 Batch 900 Loss 0.0070\n",
      "Epoch 1 Batch 1000 Loss 0.0090\n",
      "Epoch 1 Batch 1100 Loss 0.0130\n",
      "Epoch 1 Batch 1200 Loss 0.0107\n",
      "Epoch 1 Batch 1300 Loss 0.0081\n",
      "Epoch 1 Batch 1400 Loss 0.0068\n",
      "Epoch 1 Batch 1500 Loss 0.0066\n",
      "Epoch 1 Batch 1600 Loss 0.0070\n",
      "Epoch 1 Batch 1700 Loss 0.0082\n",
      "Epoch 1 Batch 1800 Loss 0.0076\n",
      "Epoch 1 Batch 1900 Loss 0.0074\n",
      "Epoch 1 Batch 2000 Loss 0.0059\n",
      "Epoch 1 Batch 2100 Loss 0.0068\n",
      "Epoch 1 Batch 2200 Loss 0.0084\n",
      "Epoch 1 Batch 2300 Loss 0.0064\n",
      "Epoch 1 Batch 2400 Loss 0.0070\n",
      "Epoch 1 Batch 2500 Loss 0.0080\n",
      "Epoch 1 Batch 2600 Loss 0.0056\n",
      "Epoch 1 Batch 2700 Loss 0.0061\n",
      "Epoch 1 Batch 2800 Loss 0.0129\n",
      "Epoch 1 Batch 2900 Loss 0.0078\n",
      "Epoch 1 Batch 3000 Loss 0.0092\n",
      "Epoch 1 Loss 0.0076\n",
      "Time taken for 1 epoch 805.990903377533 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.0070\n",
      "Epoch 2 Batch 100 Loss 0.0057\n",
      "Epoch 2 Batch 200 Loss 0.0067\n",
      "Epoch 2 Batch 300 Loss 0.0079\n",
      "Epoch 2 Batch 400 Loss 0.0065\n",
      "Epoch 2 Batch 500 Loss 0.0060\n",
      "Epoch 2 Batch 600 Loss 0.0069\n",
      "Epoch 2 Batch 700 Loss 0.0079\n",
      "Epoch 2 Batch 800 Loss 0.0092\n",
      "Epoch 2 Batch 900 Loss 0.0073\n",
      "Epoch 2 Batch 1000 Loss 0.0060\n",
      "Epoch 2 Batch 1100 Loss 0.0085\n",
      "Epoch 2 Batch 1200 Loss 0.0108\n",
      "Epoch 2 Batch 1300 Loss 0.0075\n",
      "Epoch 2 Batch 1400 Loss 0.0061\n",
      "Epoch 2 Batch 1500 Loss 0.0080\n",
      "Epoch 2 Batch 1600 Loss 0.0070\n",
      "Epoch 2 Batch 1700 Loss 0.0067\n",
      "Epoch 2 Batch 1800 Loss 0.0059\n",
      "Epoch 2 Batch 1900 Loss 0.0087\n",
      "Epoch 2 Batch 2000 Loss 0.0064\n",
      "Epoch 2 Batch 2100 Loss 0.0076\n",
      "Epoch 2 Batch 2200 Loss 0.0073\n",
      "Epoch 2 Batch 2300 Loss 0.0065\n",
      "Epoch 2 Batch 2400 Loss 0.0075\n",
      "Epoch 2 Batch 2500 Loss 0.0058\n",
      "Epoch 2 Batch 2600 Loss 0.0099\n",
      "Epoch 2 Batch 2700 Loss 0.0069\n",
      "Epoch 2 Batch 2800 Loss 0.0070\n",
      "Epoch 2 Batch 2900 Loss 0.0065\n",
      "Epoch 2 Batch 3000 Loss 0.0067\n",
      "Epoch 2 Loss 0.0072\n",
      "Time taken for 1 epoch 804.486005783081 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.0067\n",
      "Epoch 3 Batch 100 Loss 0.0087\n",
      "Epoch 3 Batch 200 Loss 0.0069\n",
      "Epoch 3 Batch 300 Loss 0.0058\n",
      "Epoch 3 Batch 400 Loss 0.0062\n",
      "Epoch 3 Batch 500 Loss 0.0057\n",
      "Epoch 3 Batch 600 Loss 0.0061\n",
      "Epoch 3 Batch 700 Loss 0.0075\n",
      "Epoch 3 Batch 800 Loss 0.0062\n",
      "Epoch 3 Batch 900 Loss 0.0063\n",
      "Epoch 3 Batch 1000 Loss 0.0066\n",
      "Epoch 3 Batch 1100 Loss 0.0067\n",
      "Epoch 3 Batch 1200 Loss 0.0061\n",
      "Epoch 3 Batch 1300 Loss 0.0062\n",
      "Epoch 3 Batch 1400 Loss 0.0060\n",
      "Epoch 3 Batch 1500 Loss 0.0072\n",
      "Epoch 3 Batch 1600 Loss 0.0059\n",
      "Epoch 3 Batch 1700 Loss 0.0065\n",
      "Epoch 3 Batch 1800 Loss 0.0059\n",
      "Epoch 3 Batch 1900 Loss 0.0073\n",
      "Epoch 3 Batch 2000 Loss 0.0068\n",
      "Epoch 3 Batch 2100 Loss 0.0088\n",
      "Epoch 3 Batch 2200 Loss 0.0059\n",
      "Epoch 3 Batch 2300 Loss 0.0056\n",
      "Epoch 3 Batch 2400 Loss 0.0052\n",
      "Epoch 3 Batch 2500 Loss 0.0075\n",
      "Epoch 3 Batch 2600 Loss 0.0070\n",
      "Epoch 3 Batch 2700 Loss 0.0064\n",
      "Epoch 3 Batch 2800 Loss 0.0065\n",
      "Epoch 3 Batch 2900 Loss 0.0066\n",
      "Epoch 3 Batch 3000 Loss 0.0053\n",
      "Epoch 3 Loss 0.0069\n",
      "Time taken for 1 epoch 804.4631795883179 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.0064\n",
      "Epoch 4 Batch 100 Loss 0.0076\n",
      "Epoch 4 Batch 200 Loss 0.0070\n",
      "Epoch 4 Batch 300 Loss 0.0065\n",
      "Epoch 4 Batch 400 Loss 0.0065\n",
      "Epoch 4 Batch 500 Loss 0.0060\n",
      "Epoch 4 Batch 600 Loss 0.0077\n",
      "Epoch 4 Batch 700 Loss 0.0068\n",
      "Epoch 4 Batch 800 Loss 0.0074\n",
      "Epoch 4 Batch 900 Loss 0.0076\n",
      "Epoch 4 Batch 1000 Loss 0.0052\n",
      "Epoch 4 Batch 1100 Loss 0.0063\n",
      "Epoch 4 Batch 1200 Loss 0.0083\n",
      "Epoch 4 Batch 1300 Loss 0.0083\n",
      "Epoch 4 Batch 1400 Loss 0.0078\n",
      "Epoch 4 Batch 1500 Loss 0.0072\n",
      "Epoch 4 Batch 1600 Loss 0.0071\n",
      "Epoch 4 Batch 1700 Loss 0.0068\n",
      "Epoch 4 Batch 1800 Loss 0.0055\n",
      "Epoch 4 Batch 1900 Loss 0.0048\n",
      "Epoch 4 Batch 2000 Loss 0.0069\n",
      "Epoch 4 Batch 2100 Loss 0.0090\n",
      "Epoch 4 Batch 2200 Loss 0.0056\n",
      "Epoch 4 Batch 2300 Loss 0.0065\n",
      "Epoch 4 Batch 2400 Loss 0.0065\n",
      "Epoch 4 Batch 2500 Loss 0.0065\n",
      "Epoch 4 Batch 2600 Loss 0.0061\n",
      "Epoch 4 Batch 2700 Loss 0.0049\n",
      "Epoch 4 Batch 2800 Loss 0.0056\n",
      "Epoch 4 Batch 2900 Loss 0.0057\n",
      "Epoch 4 Batch 3000 Loss 0.0078\n",
      "Epoch 4 Loss 0.0066\n",
      "Time taken for 1 epoch 805.1926443576813 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0055\n",
      "Epoch 5 Batch 100 Loss 0.0077\n",
      "Epoch 5 Batch 200 Loss 0.0076\n",
      "Epoch 5 Batch 300 Loss 0.0060\n",
      "Epoch 5 Batch 400 Loss 0.0069\n",
      "Epoch 5 Batch 500 Loss 0.0063\n",
      "Epoch 5 Batch 600 Loss 0.0067\n",
      "Epoch 5 Batch 700 Loss 0.0049\n",
      "Epoch 5 Batch 800 Loss 0.0088\n",
      "Epoch 5 Batch 900 Loss 0.0058\n",
      "Epoch 5 Batch 1000 Loss 0.0050\n",
      "Epoch 5 Batch 1100 Loss 0.0057\n",
      "Epoch 5 Batch 1200 Loss 0.0054\n",
      "Epoch 5 Batch 1300 Loss 0.0061\n",
      "Epoch 5 Batch 1400 Loss 0.0088\n",
      "Epoch 5 Batch 1500 Loss 0.0058\n",
      "Epoch 5 Batch 1600 Loss 0.0078\n",
      "Epoch 5 Batch 1700 Loss 0.0094\n",
      "Epoch 5 Batch 1800 Loss 0.0094\n",
      "Epoch 5 Batch 1900 Loss 0.0078\n",
      "Epoch 5 Batch 2000 Loss 0.0066\n",
      "Epoch 5 Batch 2100 Loss 0.0076\n",
      "Epoch 5 Batch 2200 Loss 0.0061\n",
      "Epoch 5 Batch 2300 Loss 0.0061\n",
      "Epoch 5 Batch 2400 Loss 0.0090\n",
      "Epoch 5 Batch 2500 Loss 0.0055\n",
      "Epoch 5 Batch 2600 Loss 0.0053\n",
      "Epoch 5 Batch 2700 Loss 0.0067\n",
      "Epoch 5 Batch 2800 Loss 0.0076\n",
      "Epoch 5 Batch 2900 Loss 0.0071\n",
      "Epoch 5 Batch 3000 Loss 0.0059\n",
      "Epoch 5 Loss 0.0067\n",
      "Time taken for 1 epoch 805.1055819988251 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0066\n",
      "Epoch 6 Batch 100 Loss 0.0051\n",
      "Epoch 6 Batch 200 Loss 0.0101\n",
      "Epoch 6 Batch 300 Loss 0.0069\n",
      "Epoch 6 Batch 400 Loss 0.0063\n",
      "Epoch 6 Batch 500 Loss 0.0059\n",
      "Epoch 6 Batch 600 Loss 0.0062\n",
      "Epoch 6 Batch 700 Loss 0.0064\n",
      "Epoch 6 Batch 800 Loss 0.0053\n",
      "Epoch 6 Batch 900 Loss 0.0057\n",
      "Epoch 6 Batch 1000 Loss 0.0064\n",
      "Epoch 6 Batch 1100 Loss 0.0057\n",
      "Epoch 6 Batch 1200 Loss 0.0054\n",
      "Epoch 6 Batch 1300 Loss 0.0094\n",
      "Epoch 6 Batch 1400 Loss 0.0073\n",
      "Epoch 6 Batch 1500 Loss 0.0055\n",
      "Epoch 6 Batch 1600 Loss 0.0053\n",
      "Epoch 6 Batch 1700 Loss 0.0060\n",
      "Epoch 6 Batch 1800 Loss 0.0078\n",
      "Epoch 6 Batch 1900 Loss 0.0075\n",
      "Epoch 6 Batch 2000 Loss 0.0065\n",
      "Epoch 6 Batch 2100 Loss 0.0058\n",
      "Epoch 6 Batch 2200 Loss 0.0050\n",
      "Epoch 6 Batch 2300 Loss 0.0057\n",
      "Epoch 6 Batch 2400 Loss 0.0051\n",
      "Epoch 6 Batch 2500 Loss 0.0060\n",
      "Epoch 6 Batch 2600 Loss 0.0056\n",
      "Epoch 6 Batch 2700 Loss 0.0054\n",
      "Epoch 6 Batch 2800 Loss 0.0059\n",
      "Epoch 6 Batch 2900 Loss 0.0070\n",
      "Epoch 6 Batch 3000 Loss 0.0061\n",
      "Epoch 6 Loss 0.0062\n",
      "Time taken for 1 epoch 803.9087829589844 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0047\n",
      "Epoch 7 Batch 100 Loss 0.0052\n",
      "Epoch 7 Batch 200 Loss 0.0107\n",
      "Epoch 7 Batch 300 Loss 0.0060\n",
      "Epoch 7 Batch 400 Loss 0.0051\n",
      "Epoch 7 Batch 500 Loss 0.0064\n",
      "Epoch 7 Batch 600 Loss 0.0053\n",
      "Epoch 7 Batch 700 Loss 0.0066\n",
      "Epoch 7 Batch 800 Loss 0.0058\n",
      "Epoch 7 Batch 900 Loss 0.0070\n",
      "Epoch 7 Batch 1000 Loss 0.0058\n",
      "Epoch 7 Batch 1100 Loss 0.0061\n",
      "Epoch 7 Batch 1200 Loss 0.0061\n",
      "Epoch 7 Batch 1300 Loss 0.0066\n",
      "Epoch 7 Batch 1400 Loss 0.0075\n",
      "Epoch 7 Batch 1500 Loss 0.0056\n",
      "Epoch 7 Batch 1600 Loss 0.0063\n",
      "Epoch 7 Batch 1700 Loss 0.0084\n",
      "Epoch 7 Batch 1800 Loss 0.0045\n",
      "Epoch 7 Batch 1900 Loss 0.0069\n",
      "Epoch 7 Batch 2000 Loss 0.0051\n",
      "Epoch 7 Batch 2100 Loss 0.0060\n",
      "Epoch 7 Batch 2200 Loss 0.0060\n",
      "Epoch 7 Batch 2300 Loss 0.0061\n",
      "Epoch 7 Batch 2400 Loss 0.0061\n",
      "Epoch 7 Batch 2500 Loss 0.0056\n",
      "Epoch 7 Batch 2600 Loss 0.0055\n",
      "Epoch 7 Batch 2700 Loss 0.0051\n",
      "Epoch 7 Batch 2800 Loss 0.0049\n",
      "Epoch 7 Batch 2900 Loss 0.0058\n",
      "Epoch 7 Batch 3000 Loss 0.0062\n",
      "Epoch 7 Loss 0.0060\n",
      "Time taken for 1 epoch 804.8595948219299 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0051\n",
      "Epoch 8 Batch 100 Loss 0.0049\n",
      "Epoch 8 Batch 200 Loss 0.0152\n",
      "Epoch 8 Batch 300 Loss 0.0063\n",
      "Epoch 8 Batch 400 Loss 0.0058\n",
      "Epoch 8 Batch 500 Loss 0.0055\n",
      "Epoch 8 Batch 600 Loss 0.0053\n",
      "Epoch 8 Batch 700 Loss 0.0047\n",
      "Epoch 8 Batch 800 Loss 0.0058\n",
      "Epoch 8 Batch 900 Loss 0.0057\n",
      "Epoch 8 Batch 1000 Loss 0.0058\n",
      "Epoch 8 Batch 1100 Loss 0.0106\n",
      "Epoch 8 Batch 1200 Loss 0.0043\n",
      "Epoch 8 Batch 1300 Loss 0.0061\n",
      "Epoch 8 Batch 1400 Loss 0.0050\n",
      "Epoch 8 Batch 1500 Loss 0.0060\n",
      "Epoch 8 Batch 1600 Loss 0.0046\n",
      "Epoch 8 Batch 1700 Loss 0.0050\n",
      "Epoch 8 Batch 1800 Loss 0.0050\n",
      "Epoch 8 Batch 1900 Loss 0.0087\n",
      "Epoch 8 Batch 2000 Loss 0.0060\n",
      "Epoch 8 Batch 2100 Loss 0.0052\n",
      "Epoch 8 Batch 2200 Loss 0.0059\n",
      "Epoch 8 Batch 2300 Loss 0.0048\n",
      "Epoch 8 Batch 2400 Loss 0.0055\n",
      "Epoch 8 Batch 2500 Loss 0.0057\n",
      "Epoch 8 Batch 2600 Loss 0.0052\n",
      "Epoch 8 Batch 2700 Loss 0.0065\n",
      "Epoch 8 Batch 2800 Loss 0.0056\n",
      "Epoch 8 Batch 2900 Loss 0.0075\n",
      "Epoch 8 Batch 3000 Loss 0.0067\n",
      "Epoch 8 Loss 0.0059\n",
      "Time taken for 1 epoch 804.2878742218018 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0058\n",
      "Epoch 9 Batch 100 Loss 0.0072\n",
      "Epoch 9 Batch 200 Loss 0.0077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 300 Loss 0.0062\n",
      "Epoch 9 Batch 400 Loss 0.0077\n",
      "Epoch 9 Batch 500 Loss 0.0049\n",
      "Epoch 9 Batch 600 Loss 0.0067\n",
      "Epoch 9 Batch 700 Loss 0.0054\n",
      "Epoch 9 Batch 800 Loss 0.0052\n",
      "Epoch 9 Batch 900 Loss 0.0047\n",
      "Epoch 9 Batch 1000 Loss 0.0053\n",
      "Epoch 9 Batch 1100 Loss 0.0055\n",
      "Epoch 9 Batch 1200 Loss 0.0044\n",
      "Epoch 9 Batch 1300 Loss 0.0053\n",
      "Epoch 9 Batch 1400 Loss 0.0045\n",
      "Epoch 9 Batch 1500 Loss 0.0067\n",
      "Epoch 9 Batch 1600 Loss 0.0062\n",
      "Epoch 9 Batch 1700 Loss 0.0055\n",
      "Epoch 9 Batch 1800 Loss 0.0050\n",
      "Epoch 9 Batch 1900 Loss 0.0046\n",
      "Epoch 9 Batch 2000 Loss 0.0076\n",
      "Epoch 9 Batch 2100 Loss 0.0050\n",
      "Epoch 9 Batch 2200 Loss 0.0043\n",
      "Epoch 9 Batch 2300 Loss 0.0058\n",
      "Epoch 9 Batch 2400 Loss 0.0054\n",
      "Epoch 9 Batch 2500 Loss 0.0049\n",
      "Epoch 9 Batch 2600 Loss 0.0073\n",
      "Epoch 9 Batch 2700 Loss 0.0054\n",
      "Epoch 9 Batch 2800 Loss 0.0054\n",
      "Epoch 9 Batch 2900 Loss 0.0054\n",
      "Epoch 9 Batch 3000 Loss 0.0046\n",
      "Epoch 9 Loss 0.0056\n",
      "Time taken for 1 epoch 804.6525082588196 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0050\n",
      "Epoch 10 Batch 100 Loss 0.0054\n",
      "Epoch 10 Batch 200 Loss 0.0056\n",
      "Epoch 10 Batch 300 Loss 0.0055\n",
      "Epoch 10 Batch 400 Loss 0.0056\n",
      "Epoch 10 Batch 500 Loss 0.0044\n",
      "Epoch 10 Batch 600 Loss 0.0064\n",
      "Epoch 10 Batch 700 Loss 0.0055\n",
      "Epoch 10 Batch 800 Loss 0.0063\n",
      "Epoch 10 Batch 900 Loss 0.0065\n",
      "Epoch 10 Batch 1000 Loss 0.0053\n",
      "Epoch 10 Batch 1100 Loss 0.0053\n",
      "Epoch 10 Batch 1200 Loss 0.0070\n",
      "Epoch 10 Batch 1300 Loss 0.0050\n",
      "Epoch 10 Batch 1400 Loss 0.0048\n",
      "Epoch 10 Batch 1500 Loss 0.0052\n",
      "Epoch 10 Batch 1600 Loss 0.0051\n",
      "Epoch 10 Batch 1700 Loss 0.0048\n",
      "Epoch 10 Batch 1800 Loss 0.0051\n",
      "Epoch 10 Batch 1900 Loss 0.0062\n",
      "Epoch 10 Batch 2000 Loss 0.0056\n",
      "Epoch 10 Batch 2100 Loss 0.0049\n",
      "Epoch 10 Batch 2200 Loss 0.0056\n",
      "Epoch 10 Batch 2300 Loss 0.0052\n",
      "Epoch 10 Batch 2400 Loss 0.0051\n",
      "Epoch 10 Batch 2500 Loss 0.0059\n",
      "Epoch 10 Batch 2600 Loss 0.0049\n",
      "Epoch 10 Batch 2700 Loss 0.0055\n",
      "Epoch 10 Batch 2800 Loss 0.0062\n",
      "Epoch 10 Batch 2900 Loss 0.0066\n",
      "Epoch 10 Batch 3000 Loss 0.0052\n",
      "Epoch 10 Loss 0.0055\n",
      "Time taken for 1 epoch 804.9108366966248 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_step(inp, targ, m,h):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoding_output, m_en,h_en = encoder(inp, m,h)\n",
    "\n",
    "\n",
    "\n",
    "        decoding_input = tf.expand_dims(inp[:,0,:], 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, m_en, h_en = decoder(decoding_input, m_en, h_en)\n",
    "            tmp = np.expand_dims(targ[:,t,:],1)\n",
    "            loss += loss_function(tmp, predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            decoding_input = tmp\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    m,h = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, inp in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, inp, m,h)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    # saving (checkpoint) the model every epochs\n",
    "\n",
    "    \n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f2588092978>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
