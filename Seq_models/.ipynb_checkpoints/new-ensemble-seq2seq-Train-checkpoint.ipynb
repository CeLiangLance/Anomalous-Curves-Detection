{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Num GPUs Available:  1\n",
      "data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#%%\n",
    "\n",
    "folder = os.path.join('..','..','short_data_4_model','20')\n",
    "\n",
    "\n",
    "data_train = np.load(os.path.join(folder, 'train_data.npz'),'r')\n",
    "data_train = data_train['arr_0']\n",
    "\n",
    "data_valid = np.load(os.path.join(folder, 'valid_data.npz'),'r')\n",
    "data_valid = data_valid['arr_0']\n",
    "'''\n",
    "data_test = np.load(os.path.join(folder, 'test_data.npz'),'r')\n",
    "data_test = data_test['arr_0']\n",
    "'''\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE =60\n",
    "N_model = 5\n",
    "units = 100\n",
    "# no use\n",
    "embedding_dim = 256\n",
    "seq_len =50\n",
    "num_examples = len(data_train)\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_train).shuffle(num_examples)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "drop_out =0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder_wrap_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Encoder Hidden state shape: (batch size, units) (60, 100)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, encoding_units, batch_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.dropout = dropout\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                     return_sequences=True,\n",
    "                                     return_state=True,\n",
    "                                     dropout=self.dropout, \n",
    "                                     recurrent_dropout=0.0,\n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x,h):\n",
    "\n",
    "        output, h = self.gru(x, initial_state = h)\n",
    "        return output, h\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units)),tf.zeros((self.batch_size,self.encoding_units))\n",
    "\n",
    "    \n",
    "class Encoder_wrap(tf.keras.Model):\n",
    "    def __init__(self, encoding_units, batch_size, dropout):\n",
    "        super(Encoder_wrap, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.dropout = dropout\n",
    "        self.en_1 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_2 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_3 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_4 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_5 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "\n",
    "        self.do_h = keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_h = keras.layers.Dense(self.encoding_units)\n",
    "    \n",
    "    def call(self, x):\n",
    "        m,h = self.en_1.initialize_hidden_state()\n",
    "        _, h_1 = self.en_1(x,h)\n",
    "        _, h_2 = self.en_1(x,h)\n",
    "        _, h_3 = self.en_1(x,h)\n",
    "        _, h_4 = self.en_1(x,h)\n",
    "        _, h_5 = self.en_1(x,h)\n",
    "\n",
    "        concatted_h = tf.keras.layers.Concatenate()([h_1, h_2, h_3, h_4, h_5])\n",
    "        \n",
    "        op_h = self.do_h(concatted_h)\n",
    "        op_h = self.fc_h(op_h)\n",
    "\n",
    "        \n",
    "        return op_h\n",
    "        \n",
    "        \n",
    "example_input_batch = np.zeros((BATCH_SIZE,seq_len,3))\n",
    "# sample input\n",
    "\n",
    "encoder = Encoder_wrap(units, BATCH_SIZE,drop_out)\n",
    "\n",
    "h = encoder(example_input_batch)\n",
    "\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(h.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (5,batch_size,1, vocab size) (5, 60, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, decoding_units, batch_size, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.dropout = dropout\n",
    "        #self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      dropout=self.dropout, \n",
    "                                      recurrent_dropout=0.0,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.do_m = keras.layers.Dropout(self.dropout)\n",
    "        self.do_h = keras.layers.Dropout(self.dropout)\n",
    "        \n",
    "    def call(self, x, h):\n",
    "        h= self.do_h(h)\n",
    "\n",
    "\n",
    "\n",
    "        rnn, h_o = self.gru(x,initial_state = h)\n",
    "\n",
    "        op = self.fc(rnn)\n",
    "\n",
    "        return op, h_o\n",
    "\n",
    "\n",
    "class Decoder_wrap(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, decoding_units, batch_size, dropout):\n",
    "        super(Decoder_wrap, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.de_0 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_1 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_2 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_3 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_4 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "    \n",
    "    def call(self, x, h):\n",
    "        op_0, h_0 = self.de_1(x[0],  h[0])\n",
    "        op_1, h_1 = self.de_1(x[1],  h[1])\n",
    "        op_2, h_2 = self.de_1(x[2],  h[2])\n",
    "        op_3, h_3 = self.de_1(x[3],  h[3])\n",
    "        op_4, h_4 = self.de_1(x[4],  h[4])\n",
    "        \n",
    "        op =tf.keras.layers.Concatenate(axis=0)([tf.expand_dims(op_0,0), tf.expand_dims(op_1,0), \n",
    "                                           tf.expand_dims(op_2,0), tf.expand_dims(op_3,0),\n",
    "                                           tf.expand_dims(op_4,0)])\n",
    "        \n",
    "        op_h = tf.keras.layers.Concatenate(axis=0)([tf.expand_dims(h_0,0), tf.expand_dims(h_1,0),\n",
    "                                              tf.expand_dims(h_2,0), tf.expand_dims(h_3,0),\n",
    "                                              tf.expand_dims(h_4,0)])\n",
    "        return op, op_h\n",
    "        \n",
    "        \n",
    "                        \n",
    "vocab_size=3\n",
    "decoder = Decoder_wrap(vocab_size,units, BATCH_SIZE, drop_out)\n",
    "m = tf.random.uniform((5,BATCH_SIZE,units))\n",
    "sample_decoder_output,  de_h = decoder(tf.random.uniform((5, BATCH_SIZE, 1,3)),m)\n",
    "print ('Decoder output shape: (5,batch_size,1, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError( reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, [0,0,0]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_dir = str(seq_len)+'full_seq2seq-sf-GRU-simple_'+str(units)+'_'+str(BATCH_SIZE)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'50full_seq2seq-sf-GRU-simple_100_60'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef expand_n_concat(ip,n):\\n    ans = tf.expand_dims(ip,0)\\n    for i in range(n-1):\\n        tmp = tf.expand_dims(ip,0)\\n        ans = tf.keras.layers.Concatenate(axis=0,dtype=tf.float32)([ans,tmp])\\n    return ans\\n        \\ninp = np.zeros((BATCH_SIZE,550,3))\\nm_en,h_en = encoder(inp)\\n\\nde_in = tf.expand_dims(inp[:,0,:], 1)\\nde_in = expand_n_concat(de_in, N_model)\\nm_en = expand_n_concat(m_en, N_model)\\nh_en = expand_n_concat(h_en, N_model)\\nprint(de_in.shape)\\nprint(h_en.shape)\\nprint(m_en.shape)\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def expand_n_concat(ip,n):\n",
    "    ans = tf.expand_dims(ip,0)\n",
    "    for i in range(n-1):\n",
    "        tmp = tf.expand_dims(ip,0)\n",
    "        ans = tf.keras.layers.Concatenate(axis=0,dtype=tf.float32)([ans,tmp])\n",
    "    return ans\n",
    "        \n",
    "inp = np.zeros((BATCH_SIZE,550,3))\n",
    "m_en,h_en = encoder(inp)\n",
    "\n",
    "de_in = tf.expand_dims(inp[:,0,:], 1)\n",
    "de_in = expand_n_concat(de_in, N_model)\n",
    "m_en = expand_n_concat(m_en, N_model)\n",
    "h_en = expand_n_concat(h_en, N_model)\n",
    "print(de_in.shape)\n",
    "print(h_en.shape)\n",
    "print(m_en.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 681.6779\n",
      "Epoch 1 Batch 100 Loss 15.9501\n",
      "Epoch 1 Batch 200 Loss 3.8906\n",
      "Epoch 1 Batch 300 Loss 4.2066\n",
      "Epoch 1 Batch 400 Loss 1.6616\n",
      "Epoch 1 Batch 500 Loss 1.4143\n",
      "Epoch 1 Batch 600 Loss 1.1556\n",
      "Epoch 1 Batch 700 Loss 0.8608\n",
      "Epoch 1 Batch 800 Loss 0.8300\n",
      "Epoch 1 Batch 900 Loss 1.3560\n",
      "Epoch 1 Batch 1000 Loss 0.6676\n",
      "Epoch 1 Batch 1100 Loss 0.5534\n",
      "Epoch 1 Batch 1200 Loss 0.6472\n",
      "Epoch 1 Batch 1300 Loss 0.5834\n",
      "Epoch 1 Batch 1400 Loss 0.4118\n",
      "Epoch 1 Batch 1500 Loss 0.4842\n",
      "Epoch 1 Batch 1600 Loss 1.0503\n",
      "Epoch 1 Batch 1700 Loss 0.5362\n",
      "Epoch 1 Batch 1800 Loss 0.5168\n",
      "Epoch 1 Batch 1900 Loss 0.4513\n",
      "Epoch 1 Batch 2000 Loss 0.5253\n",
      "Epoch 1 Batch 2100 Loss 0.4869\n",
      "Epoch 1 Batch 2200 Loss 0.3695\n",
      "Epoch 1 Batch 2300 Loss 0.2992\n",
      "Epoch 1 Batch 2400 Loss 0.3688\n",
      "Epoch 1 Batch 2500 Loss 0.3112\n",
      "Epoch 1 Batch 2600 Loss 0.3539\n",
      "Epoch 1 Batch 2700 Loss 0.3326\n",
      "Epoch 1 Batch 2800 Loss 0.2565\n",
      "Epoch 1 Batch 2900 Loss 0.3277\n",
      "Epoch 1 Batch 3000 Loss 0.2997\n",
      "Epoch 1 Loss 5.1003\n",
      "Time taken for 1 epoch 2403.59214758873 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.2857\n",
      "Epoch 2 Batch 100 Loss 0.3672\n",
      "Epoch 2 Batch 200 Loss 0.3258\n",
      "Epoch 2 Batch 300 Loss 0.2553\n",
      "Epoch 2 Batch 400 Loss 0.2704\n",
      "Epoch 2 Batch 500 Loss 0.2848\n",
      "Epoch 2 Batch 600 Loss 0.2632\n",
      "Epoch 2 Batch 700 Loss 0.3078\n",
      "Epoch 2 Batch 800 Loss 0.3644\n",
      "Epoch 2 Batch 900 Loss 0.2856\n",
      "Epoch 2 Batch 1000 Loss 0.2607\n",
      "Epoch 2 Batch 1100 Loss 0.2387\n",
      "Epoch 2 Batch 1200 Loss 0.2099\n",
      "Epoch 2 Batch 1300 Loss 0.2514\n",
      "Epoch 2 Batch 1400 Loss 0.2914\n",
      "Epoch 2 Batch 1500 Loss 0.1870\n",
      "Epoch 2 Batch 1600 Loss 0.1980\n",
      "Epoch 2 Batch 1700 Loss 0.2094\n",
      "Epoch 2 Batch 1800 Loss 0.2099\n",
      "Epoch 2 Batch 1900 Loss 0.4756\n",
      "Epoch 2 Batch 2000 Loss 0.2710\n",
      "Epoch 2 Batch 2100 Loss 0.2403\n",
      "Epoch 2 Batch 2200 Loss 0.1747\n",
      "Epoch 2 Batch 2300 Loss 0.1976\n",
      "Epoch 2 Batch 2400 Loss 0.1990\n",
      "Epoch 2 Batch 2500 Loss 0.2201\n",
      "Epoch 2 Batch 2600 Loss 0.2021\n",
      "Epoch 2 Batch 2700 Loss 0.2291\n",
      "Epoch 2 Batch 2800 Loss 0.1847\n",
      "Epoch 2 Batch 2900 Loss 0.1599\n",
      "Epoch 2 Batch 3000 Loss 0.1826\n",
      "Epoch 2 Loss 0.2594\n",
      "Time taken for 1 epoch 2390.472995519638 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.2081\n",
      "Epoch 3 Batch 100 Loss 0.1829\n",
      "Epoch 3 Batch 200 Loss 0.1466\n",
      "Epoch 3 Batch 300 Loss 0.2174\n",
      "Epoch 3 Batch 400 Loss 0.1468\n",
      "Epoch 3 Batch 500 Loss 0.1919\n",
      "Epoch 3 Batch 600 Loss 0.1942\n",
      "Epoch 3 Batch 700 Loss 0.1570\n",
      "Epoch 3 Batch 800 Loss 0.1593\n",
      "Epoch 3 Batch 900 Loss 0.3325\n",
      "Epoch 3 Batch 1000 Loss 0.1329\n",
      "Epoch 3 Batch 1100 Loss 0.1474\n",
      "Epoch 3 Batch 1200 Loss 0.1681\n",
      "Epoch 3 Batch 1300 Loss 0.1592\n",
      "Epoch 3 Batch 1400 Loss 0.2255\n",
      "Epoch 3 Batch 1500 Loss 0.1196\n",
      "Epoch 3 Batch 1600 Loss 0.1439\n",
      "Epoch 3 Batch 1700 Loss 0.1521\n",
      "Epoch 3 Batch 1800 Loss 0.1236\n",
      "Epoch 3 Batch 1900 Loss 0.1447\n",
      "Epoch 3 Batch 2000 Loss 0.1538\n",
      "Epoch 3 Batch 2100 Loss 0.1978\n",
      "Epoch 3 Batch 2200 Loss 0.2190\n",
      "Epoch 3 Batch 2300 Loss 0.1214\n",
      "Epoch 3 Batch 2400 Loss 0.1359\n",
      "Epoch 3 Batch 2500 Loss 1.4239\n",
      "Epoch 3 Batch 2600 Loss 0.1441\n",
      "Epoch 3 Batch 2700 Loss 0.1262\n",
      "Epoch 3 Batch 2800 Loss 0.1318\n",
      "Epoch 3 Batch 2900 Loss 0.1334\n",
      "Epoch 3 Batch 3000 Loss 0.1299\n",
      "Epoch 3 Loss 0.1671\n",
      "Time taken for 1 epoch 2391.6438162326813 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.1767\n",
      "Epoch 4 Batch 100 Loss 0.1416\n",
      "Epoch 4 Batch 200 Loss 0.1110\n",
      "Epoch 4 Batch 300 Loss 0.1356\n",
      "Epoch 4 Batch 400 Loss 0.1214\n",
      "Epoch 4 Batch 500 Loss 0.1289\n",
      "Epoch 4 Batch 600 Loss 0.1696\n",
      "Epoch 4 Batch 700 Loss 0.1218\n",
      "Epoch 4 Batch 800 Loss 0.1278\n",
      "Epoch 4 Batch 900 Loss 0.1135\n",
      "Epoch 4 Batch 1000 Loss 0.1244\n",
      "Epoch 4 Batch 1100 Loss 0.1138\n",
      "Epoch 4 Batch 1200 Loss 0.1176\n",
      "Epoch 4 Batch 1300 Loss 0.1326\n",
      "Epoch 4 Batch 1400 Loss 0.3041\n",
      "Epoch 4 Batch 1500 Loss 0.1103\n",
      "Epoch 4 Batch 1600 Loss 0.1212\n",
      "Epoch 4 Batch 1700 Loss 0.1247\n",
      "Epoch 4 Batch 1800 Loss 0.1119\n",
      "Epoch 4 Batch 1900 Loss 0.1030\n",
      "Epoch 4 Batch 2000 Loss 0.1387\n",
      "Epoch 4 Batch 2100 Loss 0.1304\n",
      "Epoch 4 Batch 2200 Loss 0.1076\n",
      "Epoch 4 Batch 2300 Loss 0.1097\n",
      "Epoch 4 Batch 2400 Loss 0.1035\n",
      "Epoch 4 Batch 2500 Loss 0.1011\n",
      "Epoch 4 Batch 2600 Loss 0.1024\n",
      "Epoch 4 Batch 2700 Loss 0.1067\n",
      "Epoch 4 Batch 2800 Loss 0.1265\n",
      "Epoch 4 Batch 2900 Loss 0.0930\n",
      "Epoch 4 Batch 3000 Loss 0.1014\n",
      "Epoch 4 Loss 0.1236\n",
      "Time taken for 1 epoch 2389.876886844635 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.0928\n",
      "Epoch 5 Batch 100 Loss 0.1684\n",
      "Epoch 5 Batch 200 Loss 0.1077\n",
      "Epoch 5 Batch 300 Loss 0.0961\n",
      "Epoch 5 Batch 400 Loss 0.1140\n",
      "Epoch 5 Batch 500 Loss 0.1135\n",
      "Epoch 5 Batch 600 Loss 0.0886\n",
      "Epoch 5 Batch 700 Loss 0.1062\n",
      "Epoch 5 Batch 800 Loss 0.1005\n",
      "Epoch 5 Batch 900 Loss 0.0897\n",
      "Epoch 5 Batch 1000 Loss 0.0979\n",
      "Epoch 5 Batch 1100 Loss 0.1069\n",
      "Epoch 5 Batch 1200 Loss 0.1042\n",
      "Epoch 5 Batch 1300 Loss 0.0942\n",
      "Epoch 5 Batch 1400 Loss 0.1145\n",
      "Epoch 5 Batch 1500 Loss 0.0888\n",
      "Epoch 5 Batch 1600 Loss 0.0781\n",
      "Epoch 5 Batch 1700 Loss 0.1003\n",
      "Epoch 5 Batch 1800 Loss 0.1060\n",
      "Epoch 5 Batch 1900 Loss 0.1056\n",
      "Epoch 5 Batch 2000 Loss 0.0957\n",
      "Epoch 5 Batch 2100 Loss 0.1122\n",
      "Epoch 5 Batch 2200 Loss 0.1266\n",
      "Epoch 5 Batch 2300 Loss 0.0879\n",
      "Epoch 5 Batch 2400 Loss 0.1356\n",
      "Epoch 5 Batch 2500 Loss 0.0809\n",
      "Epoch 5 Batch 2600 Loss 0.0675\n",
      "Epoch 5 Batch 2700 Loss 0.0901\n",
      "Epoch 5 Batch 2800 Loss 0.0990\n",
      "Epoch 5 Batch 2900 Loss 0.0922\n",
      "Epoch 5 Batch 3000 Loss 0.0888\n",
      "Epoch 5 Loss 0.1026\n",
      "Time taken for 1 epoch 2358.846309185028 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.0869\n",
      "Epoch 6 Batch 100 Loss 0.1094\n",
      "Epoch 6 Batch 200 Loss 0.0980\n",
      "Epoch 6 Batch 300 Loss 0.0814\n",
      "Epoch 6 Batch 400 Loss 0.1040\n",
      "Epoch 6 Batch 500 Loss 0.0831\n",
      "Epoch 6 Batch 600 Loss 0.1005\n",
      "Epoch 6 Batch 700 Loss 0.0912\n",
      "Epoch 6 Batch 800 Loss 0.0988\n",
      "Epoch 6 Batch 900 Loss 0.0863\n",
      "Epoch 6 Batch 1000 Loss 0.0974\n",
      "Epoch 6 Batch 1100 Loss 0.0908\n",
      "Epoch 6 Batch 1200 Loss 0.0766\n",
      "Epoch 6 Batch 1300 Loss 0.0810\n",
      "Epoch 6 Batch 1400 Loss 0.0756\n",
      "Epoch 6 Batch 1500 Loss 0.0883\n",
      "Epoch 6 Batch 1600 Loss 0.1018\n",
      "Epoch 6 Batch 1700 Loss 0.0892\n",
      "Epoch 6 Batch 1800 Loss 0.0846\n",
      "Epoch 6 Batch 1900 Loss 0.0981\n",
      "Epoch 6 Batch 2000 Loss 0.0966\n",
      "Epoch 6 Batch 2100 Loss 0.0826\n",
      "Epoch 6 Batch 2200 Loss 0.0703\n",
      "Epoch 6 Batch 2300 Loss 0.0735\n",
      "Epoch 6 Batch 2400 Loss 0.1065\n",
      "Epoch 6 Batch 2500 Loss 0.1103\n",
      "Epoch 6 Batch 2600 Loss 0.0878\n",
      "Epoch 6 Batch 2700 Loss 0.0822\n",
      "Epoch 6 Batch 2800 Loss 0.1088\n",
      "Epoch 6 Batch 2900 Loss 0.0954\n",
      "Epoch 6 Batch 3000 Loss 0.0904\n",
      "Epoch 6 Loss 0.0879\n",
      "Time taken for 1 epoch 2385.4360637664795 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.0826\n",
      "Epoch 7 Batch 100 Loss 0.0767\n",
      "Epoch 7 Batch 200 Loss 0.1007\n",
      "Epoch 7 Batch 300 Loss 0.0842\n",
      "Epoch 7 Batch 400 Loss 0.1177\n",
      "Epoch 7 Batch 500 Loss 0.0997\n",
      "Epoch 7 Batch 600 Loss 0.0663\n",
      "Epoch 7 Batch 700 Loss 0.0700\n",
      "Epoch 7 Batch 800 Loss 0.0776\n",
      "Epoch 7 Batch 900 Loss 0.0704\n",
      "Epoch 7 Batch 1000 Loss 0.0909\n",
      "Epoch 7 Batch 1100 Loss 0.1116\n",
      "Epoch 7 Batch 1200 Loss 0.0635\n",
      "Epoch 7 Batch 1300 Loss 0.0770\n",
      "Epoch 7 Batch 1400 Loss 0.0828\n",
      "Epoch 7 Batch 1500 Loss 0.0823\n",
      "Epoch 7 Batch 1600 Loss 0.0725\n",
      "Epoch 7 Batch 1700 Loss 0.0690\n",
      "Epoch 7 Batch 1800 Loss 0.0675\n",
      "Epoch 7 Batch 1900 Loss 0.0872\n",
      "Epoch 7 Batch 2000 Loss 0.0854\n",
      "Epoch 7 Batch 2100 Loss 0.0715\n",
      "Epoch 7 Batch 2200 Loss 0.0772\n",
      "Epoch 7 Batch 2300 Loss 0.0626\n",
      "Epoch 7 Batch 2400 Loss 0.0866\n",
      "Epoch 7 Batch 2500 Loss 0.0809\n",
      "Epoch 7 Batch 2600 Loss 0.0727\n",
      "Epoch 7 Batch 2700 Loss 0.0913\n",
      "Epoch 7 Batch 2800 Loss 0.0759\n",
      "Epoch 7 Batch 2900 Loss 0.0994\n",
      "Epoch 7 Batch 3000 Loss 0.0648\n",
      "Epoch 7 Loss 0.0777\n",
      "Time taken for 1 epoch 2386.0467524528503 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.1640\n",
      "Epoch 8 Batch 100 Loss 0.0602\n",
      "Epoch 8 Batch 200 Loss 0.0637\n",
      "Epoch 8 Batch 300 Loss 0.0944\n",
      "Epoch 8 Batch 400 Loss 0.0616\n",
      "Epoch 8 Batch 500 Loss 0.0603\n",
      "Epoch 8 Batch 600 Loss 0.0575\n",
      "Epoch 8 Batch 700 Loss 0.0658\n",
      "Epoch 8 Batch 800 Loss 0.0729\n",
      "Epoch 8 Batch 900 Loss 0.0700\n",
      "Epoch 8 Batch 1000 Loss 0.0667\n",
      "Epoch 8 Batch 1100 Loss 0.0686\n",
      "Epoch 8 Batch 1200 Loss 0.0710\n",
      "Epoch 8 Batch 1300 Loss 0.0604\n",
      "Epoch 8 Batch 1400 Loss 0.0712\n",
      "Epoch 8 Batch 1500 Loss 0.0702\n",
      "Epoch 8 Batch 1600 Loss 0.0746\n",
      "Epoch 8 Batch 1700 Loss 0.0685\n",
      "Epoch 8 Batch 1800 Loss 0.0766\n",
      "Epoch 8 Batch 1900 Loss 0.0661\n",
      "Epoch 8 Batch 2000 Loss 0.0712\n",
      "Epoch 8 Batch 2100 Loss 0.0666\n",
      "Epoch 8 Batch 2200 Loss 0.0623\n",
      "Epoch 8 Batch 2300 Loss 0.0630\n",
      "Epoch 8 Batch 2400 Loss 0.0609\n",
      "Epoch 8 Batch 2500 Loss 0.0544\n",
      "Epoch 8 Batch 2600 Loss 0.0597\n",
      "Epoch 8 Batch 2700 Loss 0.0724\n",
      "Epoch 8 Batch 2800 Loss 0.0803\n",
      "Epoch 8 Batch 2900 Loss 0.0611\n",
      "Epoch 8 Batch 3000 Loss 0.0772\n",
      "Epoch 8 Loss 0.0696\n",
      "Time taken for 1 epoch 2396.6325030326843 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.0708\n",
      "Epoch 9 Batch 100 Loss 0.0979\n",
      "Epoch 9 Batch 200 Loss 0.0769\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch 300 Loss 0.0502\n",
      "Epoch 9 Batch 400 Loss 0.0497\n",
      "Epoch 9 Batch 500 Loss 0.0687\n",
      "Epoch 9 Batch 600 Loss 0.0768\n",
      "Epoch 9 Batch 700 Loss 0.0531\n",
      "Epoch 9 Batch 800 Loss 0.0508\n",
      "Epoch 9 Batch 900 Loss 0.0593\n",
      "Epoch 9 Batch 1000 Loss 0.0793\n",
      "Epoch 9 Batch 1100 Loss 0.0767\n",
      "Epoch 9 Batch 1200 Loss 0.0511\n",
      "Epoch 9 Batch 1300 Loss 0.0643\n",
      "Epoch 9 Batch 1400 Loss 0.0751\n",
      "Epoch 9 Batch 1500 Loss 0.0563\n",
      "Epoch 9 Batch 1600 Loss 0.0553\n",
      "Epoch 9 Batch 1700 Loss 0.0570\n",
      "Epoch 9 Batch 1800 Loss 0.0624\n",
      "Epoch 9 Batch 1900 Loss 0.0585\n",
      "Epoch 9 Batch 2000 Loss 0.0545\n",
      "Epoch 9 Batch 2100 Loss 0.0649\n",
      "Epoch 9 Batch 2200 Loss 0.0543\n",
      "Epoch 9 Batch 2300 Loss 0.0674\n",
      "Epoch 9 Batch 2400 Loss 0.0531\n",
      "Epoch 9 Batch 2500 Loss 0.0535\n",
      "Epoch 9 Batch 2600 Loss 0.0598\n",
      "Epoch 9 Batch 2700 Loss 0.0506\n",
      "Epoch 9 Batch 2800 Loss 0.0624\n",
      "Epoch 9 Batch 2900 Loss 0.0604\n",
      "Epoch 9 Batch 3000 Loss 0.0592\n",
      "Epoch 9 Loss 0.0630\n",
      "Time taken for 1 epoch 2374.8574776649475 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.0614\n",
      "Epoch 10 Batch 100 Loss 0.0532\n",
      "Epoch 10 Batch 200 Loss 0.0686\n",
      "Epoch 10 Batch 300 Loss 0.0558\n",
      "Epoch 10 Batch 400 Loss 0.0577\n",
      "Epoch 10 Batch 500 Loss 0.0542\n",
      "Epoch 10 Batch 600 Loss 0.0633\n",
      "Epoch 10 Batch 700 Loss 0.0444\n",
      "Epoch 10 Batch 800 Loss 0.0827\n",
      "Epoch 10 Batch 900 Loss 0.0476\n",
      "Epoch 10 Batch 1000 Loss 0.0712\n",
      "Epoch 10 Batch 1100 Loss 0.0764\n",
      "Epoch 10 Batch 1200 Loss 0.0603\n",
      "Epoch 10 Batch 1300 Loss 0.0575\n",
      "Epoch 10 Batch 1400 Loss 0.0523\n",
      "Epoch 10 Batch 1500 Loss 0.0514\n",
      "Epoch 10 Batch 1600 Loss 0.0524\n",
      "Epoch 10 Batch 1700 Loss 0.0548\n",
      "Epoch 10 Batch 1800 Loss 0.0475\n",
      "Epoch 10 Batch 1900 Loss 0.0531\n",
      "Epoch 10 Batch 2000 Loss 0.0645\n",
      "Epoch 10 Batch 2100 Loss 0.0680\n",
      "Epoch 10 Batch 2200 Loss 0.0593\n",
      "Epoch 10 Batch 2300 Loss 0.0578\n",
      "Epoch 10 Batch 2400 Loss 0.0523\n",
      "Epoch 10 Batch 2500 Loss 0.0527\n",
      "Epoch 10 Batch 2600 Loss 0.0513\n",
      "Epoch 10 Batch 2700 Loss 0.0515\n",
      "Epoch 10 Batch 2800 Loss 0.0522\n",
      "Epoch 10 Batch 2900 Loss 0.0591\n",
      "Epoch 10 Batch 3000 Loss 0.0427\n",
      "Epoch 10 Loss 0.0586\n",
      "Time taken for 1 epoch 2384.06595826149 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def expand_n_concat(ip,n):\n",
    "    ans = tf.expand_dims(ip,0)\n",
    "    for i in range(n-1):\n",
    "        tmp = tf.expand_dims(ip,0)\n",
    "        ans = tf.keras.layers.Concatenate(axis=0,dtype=tf.float32)([ans,tmp])\n",
    "    return ans\n",
    "\n",
    "\n",
    "\n",
    "def train_step(inp, targ):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        h_en = encoder(inp)\n",
    "\n",
    "        de_in = tf.expand_dims(inp[:,0,:], 1)\n",
    "        de_in = expand_n_concat(de_in, N_model)\n",
    "\n",
    "        h_en = expand_n_concat(h_en, N_model)\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, h_en = decoder(de_in, h_en)\n",
    "            tmp = expand_n_concat(np.expand_dims(targ[:,t,:],1),N_model)\n",
    "            loss += loss_function(tmp, predictions)\n",
    "\n",
    "            # using teacher forcing\n",
    "            de_in = tmp\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch, inp in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, inp)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "        #if batch % 1000 ==0:\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "\n",
    "    \n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
