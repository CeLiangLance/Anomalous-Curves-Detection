{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Num GPUs Available:  0\n",
      "data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#%%\n",
    "\n",
    "folder = os.path.join('..','..','short_data_4_model','20')\n",
    "\n",
    "\n",
    "data_train = np.load(os.path.join(folder, 'train_data.npz'),'r')\n",
    "data_train = data_train['arr_0']\n",
    "\n",
    "data_valid = np.load(os.path.join(folder, 'valid_data.npz'),'r')\n",
    "data_valid = data_valid['arr_0']\n",
    "'''\n",
    "data_test = np.load(os.path.join(folder, 'test_data.npz'),'r')\n",
    "data_test = data_test['arr_0']\n",
    "'''\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "BATCH_SIZE =60\n",
    "N_model = 5\n",
    "units = 100\n",
    "# no use\n",
    "embedding_dim = 256\n",
    "seq_len =50\n",
    "num_examples = len(data_train)\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n",
    "\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_train).shuffle(num_examples)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "drop_out =0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder_wrap is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Encoder Hidden state shape: (batch size, units) (60, 100)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, encoding_units, batch_size, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.dropout = dropout\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                     return_sequences=True,\n",
    "                                     return_state=True,\n",
    "                                     dropout=self.dropout, \n",
    "                                     recurrent_dropout=0.0,\n",
    "                                     recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x,h):\n",
    "\n",
    "        output, h = self.gru(x, initial_state = h)\n",
    "        return output, h\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units)),tf.zeros((self.batch_size,self.encoding_units))\n",
    "\n",
    "    \n",
    "class Encoder_wrap(tf.keras.Model):\n",
    "    def __init__(self, encoding_units, batch_size, dropout):\n",
    "        super(Encoder_wrap, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.dropout = dropout\n",
    "        self.en_1 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_2 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_3 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_4 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "        self.en_5 = Encoder(self.encoding_units, self.batch_size, self.dropout)\n",
    "\n",
    "        self.do_h = keras.layers.Dropout(self.dropout)\n",
    "\n",
    "        self.fc_h = keras.layers.Dense(self.encoding_units)\n",
    "    \n",
    "    def call(self, x):\n",
    "        m,h = self.en_1.initialize_hidden_state()\n",
    "        _, h_1 = self.en_1(x,h)\n",
    "        _, h_2 = self.en_1(x,h)\n",
    "        _, h_3 = self.en_1(x,h)\n",
    "        _, h_4 = self.en_1(x,h)\n",
    "        _, h_5 = self.en_1(x,h)\n",
    "\n",
    "        concatted_h = tf.keras.layers.Concatenate()([h_1, h_2, h_3, h_4, h_5])\n",
    "        \n",
    "        op_h = self.do_h(concatted_h)\n",
    "        op_h = self.fc_h(op_h)\n",
    "\n",
    "        \n",
    "        return op_h\n",
    "        \n",
    "        \n",
    "example_input_batch = np.zeros((BATCH_SIZE,seq_len,3))\n",
    "# sample input\n",
    "\n",
    "encoder = Encoder_wrap(units, BATCH_SIZE,drop_out)\n",
    "\n",
    "h = encoder(example_input_batch)\n",
    "\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(h.shape))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (5,batch_size,1, vocab size) (5, 60, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, decoding_units, batch_size, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.dropout = dropout\n",
    "        #self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      dropout=self.dropout, \n",
    "                                      recurrent_dropout=0.0,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        self.do_m = keras.layers.Dropout(self.dropout)\n",
    "        self.do_h = keras.layers.Dropout(self.dropout)\n",
    "        \n",
    "    def call(self, x, h):\n",
    "        h= self.do_h(h)\n",
    "\n",
    "\n",
    "\n",
    "        rnn, h_o = self.gru(x,initial_state = h)\n",
    "\n",
    "        op = self.fc(rnn)\n",
    "\n",
    "        return op, h_o\n",
    "\n",
    "\n",
    "class Decoder_wrap(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, decoding_units, batch_size, dropout):\n",
    "        super(Decoder_wrap, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        self.dropout = dropout\n",
    "        self.vocab_size = vocab_size\n",
    "        self.de_0 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_1 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_2 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_3 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "        self.de_4 = Decoder(self.vocab_size,self.decoding_units, self.batch_size, self.dropout )\n",
    "    \n",
    "    def call(self, x, h):\n",
    "        op_0, h_0 = self.de_1(x[0],  h[0])\n",
    "        op_1, h_1 = self.de_1(x[1],  h[1])\n",
    "        op_2, h_2 = self.de_1(x[2],  h[2])\n",
    "        op_3, h_3 = self.de_1(x[3],  h[3])\n",
    "        op_4, h_4 = self.de_1(x[4],  h[4])\n",
    "        \n",
    "        op =tf.keras.layers.Concatenate(axis=0)([tf.expand_dims(op_0,0), tf.expand_dims(op_1,0), \n",
    "                                           tf.expand_dims(op_2,0), tf.expand_dims(op_3,0),\n",
    "                                           tf.expand_dims(op_4,0)])\n",
    "        \n",
    "        op_h = tf.keras.layers.Concatenate(axis=0)([tf.expand_dims(h_0,0), tf.expand_dims(h_1,0),\n",
    "                                              tf.expand_dims(h_2,0), tf.expand_dims(h_3,0),\n",
    "                                              tf.expand_dims(h_4,0)])\n",
    "        return op, op_h\n",
    "        \n",
    "        \n",
    "                        \n",
    "vocab_size=3\n",
    "decoder = Decoder_wrap(vocab_size,units, BATCH_SIZE, drop_out)\n",
    "m = tf.random.uniform((5,BATCH_SIZE,units))\n",
    "sample_decoder_output,  de_h = decoder(tf.random.uniform((5, BATCH_SIZE, 1,3)),m)\n",
    "print ('Decoder output shape: (5,batch_size,1, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError( reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, [0,0,0]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "checkpoint_dir = str(seq_len)+'full_seq2seq-sf-GRU-simple_'+str(units)+'_'+str(BATCH_SIZE)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_n_concat(ip,n):\n",
    "    ans = tf.expand_dims(ip,0)\n",
    "    for i in range(n-1):\n",
    "        tmp = tf.expand_dims(ip,0)\n",
    "        ans = tf.keras.layers.Concatenate(axis=0,dtype=tf.float32)([ans,tmp])\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x1ed4e929748>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evalute_batch(inputs):\n",
    "    result = np.zeros((np.shape(inputs)))\n",
    "    inp = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    inference_batch_size = inp.shape[0]\n",
    "    h_en= encoder(inp)\n",
    "    de_in = tf.expand_dims(inp[:,0,:], 1)\n",
    "    de_in = expand_n_concat(de_in, N_model)\n",
    "\n",
    "    h_en = expand_n_concat(h_en, N_model)\n",
    "    for t in range(inp.shape[1]):\n",
    "        predictions, h_en = decoder(de_in, h_en)\n",
    "        tmp = np.mean(predictions,axis=0)\n",
    "        tmp = tf.squeeze(tmp)\n",
    "        result[:,t,:] = tmp\n",
    "        de_in = predictions\n",
    "    return result\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_subj(inputs):\n",
    "    result = np.zeros((np.shape(inputs)))\n",
    "    bz = BATCH_SIZE\n",
    "    num = np.shape(inputs)[0] // BATCH_SIZE\n",
    "    for i in range(num):\n",
    "        in_tmp = inputs[i*bz:(i+1)*bz]\n",
    "        res_tmp = evalute_batch(in_tmp)\n",
    "        result[i*bz:(i+1)*bz] = res_tmp\n",
    "        \n",
    "    if np.shape(inputs)[0] % BATCH_SIZE:\n",
    "        rem = np.shape(inputs)[0] % BATCH_SIZE\n",
    "        in_tmp = np.zeros((np.shape(res_tmp)))\n",
    "        in_tmp[0:rem] = inputs[num*bz:]\n",
    "        res_tmp = evalute_batch(in_tmp)\n",
    "        result[num*bz:] = res_tmp[0:rem]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def cal_mse(data_raw,data_pred):\n",
    "    mse = np.zeros(np.shape(data_raw)[0])\n",
    "    for i in range(np.shape(data_raw)[0]):\n",
    "        result = mean_squared_error(data_raw[i], data_pred[i])\n",
    "        mse[i] = result\n",
    "    return mse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.tracking.streamlinespeed import length\n",
    "                                          \n",
    "\n",
    "class PlyStruct:\n",
    "    \"\"\"Class that process poly data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ply_data = None\n",
    "        self.idx = None\n",
    "        self.properties = None\n",
    "        self.stream_line = None\n",
    "\n",
    "    def load_poly_data(self, subpath: str, num_prop: int = None):\n",
    "        \"\"\"\n",
    "        Load ply file\n",
    "        :param str subpath: path of ply file\n",
    "        :param int num_prop: optional parmeter to only take the first num properties (e.g. only x,y,z coordinates)\n",
    "        \"\"\"\n",
    "        ply = PlyData.read(subpath)\n",
    "        property_list = [ply['vertices'].data[name] for name in ply['vertices'].data.dtype.names]\n",
    "        self.ply_data = np.array(property_list).T\n",
    "        if num_prop:\n",
    "            self.ply_data = self.ply_data[:, :num_prop]\n",
    "        self.idx = ply['fiber'].data['endindex']\n",
    "        self.properties = ply['vertices'].data.dtype.names\n",
    "        if num_prop:\n",
    "            self.properties = self.properties[:num_prop]\n",
    "   \n",
    "\n",
    "    def gen_stream_line(self):\n",
    "        self.stream_line = []\n",
    "        self.stream_line.append(self.ply_data[0:self.idx[0],:])\n",
    "        for i in range(len(self.idx)-1):\n",
    "            self.stream_line.append(self.ply_data[self.idx[i]:self.idx[i+1],:])\n",
    "\n",
    "\n",
    "def zero_remove(darray):\n",
    "    \n",
    "    for i in range(np.shape(darray)[0]-1,-1,-1):\n",
    "        if not (np.around(darray[i], decimals=0) == 0 ).all():\n",
    "            return darray[0:i+1] \n",
    "            break\n",
    "\n",
    "\n",
    "def get_length(stream):\n",
    "    # to get the length of streams in this subject\n",
    "    len_table = []\n",
    "    for i in range(len(stream)):\n",
    "        len_table.append(np.shape(stream[i])[0])\n",
    "    return len_table\n",
    "\n",
    "def cal_mse(data_raw,data_pred):\n",
    "    mse = np.zeros(np.shape(data_raw)[0])\n",
    "    for i in range(np.shape(data_raw)[0]):\n",
    "        result = mean_squared_error(data_raw[i], data_pred[i])\n",
    "        mse[i] = result\n",
    "    return mse  \n",
    "\n",
    "\n",
    "def ply2np(name):\n",
    "    # convert a ply format to the matrix we are using\n",
    "    # input : '128127_ex_cc-body_shore.ply', name of a oly file\n",
    "    # optput: a matrix (#_of_fibers, #_of_vertex, 3 )\n",
    "    temo = PlyStruct()\n",
    "    temo.load_poly_data(os.path.join('..','data',name),num_prop=3)\n",
    "    #data = temo.ply_data\n",
    "  \n",
    "    temo.gen_stream_line()\n",
    "    stream = temo.stream_line\n",
    "\n",
    "\n",
    "    stream = np.array(stream)\n",
    "    len_table = get_length(stream)\n",
    "    subject = np.zeros((len(stream),np.max(len_table),3))\n",
    "    for i in range(len(stream)):\n",
    "        length = len_table[i]\n",
    "        subject[i,0:length,:] = stream[i]\n",
    "        \n",
    "    return subject\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def cal_mse(data_raw,data_pred):\n",
    "    mse = np.zeros(np.shape(data_raw)[0])\n",
    "    for i in range(np.shape(data_raw)[0]):\n",
    "        result = mean_squared_error(data_raw[i], data_pred[i])\n",
    "        mse[i] = result\n",
    "    return mse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: 13.80529398659342\n",
      "2: 10.966595235578795\n",
      "3: 10.344658584767998\n",
      "4: 11.02308766422285\n",
      "5: 8.813155492361354\n",
      "6: 9.53661526227147\n",
      "7: 14.774001076786943\n",
      "8: 11.111364702306044\n",
      "9: 10.661900548514183\n",
      "10: 8.142202071792822\n"
     ]
    }
   ],
   "source": [
    "# A way to calculate the validate error\n",
    "'''\n",
    "for i in range(10):\n",
    "    f = open(os.path.join(checkpoint_dir,'checkpoint'), 'w')\n",
    "    f.write('model_checkpoint_path: \"ckpt-'+str(i+1)+'\"\\n')\n",
    "    f.write('all_model_checkpoint_paths: \"ckpt-'+str(i+1)+'\"\\n')\n",
    "    f.close()\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "    result = evalute_subj(data_valid)\n",
    "    mse_idv =loss_function(data_valid, result)\n",
    "    print(str(i+1)+': '+str(mse_idv.numpy()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type in the parameters\n",
    "name : the file name\\\n",
    "len_thre: length control value, length below it will be considered abnomal fiber\\\n",
    "thre_con: control value of threshold percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '156031_ex_cc-body_shore.ply'\n",
    "len_thre = 40\n",
    "thre_con = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = ply2np(name)\n",
    "\n",
    "from dipy.tracking.streamlinespeed import (compress_streamlines, length,\n",
    "                                           set_number_of_points)\n",
    "streamlines_evl = Streamlines()\n",
    "for j in range(np.shape(bundle)[0]):\n",
    "    tmp = bundle[j]\n",
    "    tmp = zero_remove(tmp)\n",
    "    streamlines_evl.append(tmp)\n",
    "\n",
    "\n",
    "subsamp_sls = set_number_of_points(streamlines_evl, seq_len)\n",
    "\n",
    "data_test= np.array(subsamp_sls)\n",
    "result = evalute_subj(data_test)\n",
    "\n",
    "mse_idv = cal_mse(data_test, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = np.percentile(mse_idv,thre_con)\n",
    "pred_1 = 1*(mse_idv<thre)\n",
    "\n",
    "\n",
    "bundle_str = Streamlines()\n",
    "\n",
    "for i in range(np.shape(bundle)[0]):\n",
    "    tmp = bundle[i]\n",
    "    tmp = zero_remove(tmp)\n",
    "    #tmp = tmp[~np.all(tmp == 0, axis=-1)]\n",
    "    #tmp = np.around(tmp, decimals=0)\n",
    "    bundle_str.append(tmp)\n",
    "    \n",
    "lengths = length(bundle_str)\n",
    "\n",
    "pred_2 = 1*(lengths > len_thre)\n",
    "\n",
    "pred = 1*((pred_1 + pred_2)>1)\n",
    "\n",
    "op_path = 'result'\n",
    "if not os.path.exists(op_path):\n",
    "    os.mkdir(op_path)\n",
    "\n",
    "save_path = os.path.join(op_path,'Detection'+name.split('_')[0]+'_ens_seq')\n",
    "np.save(save_path,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
