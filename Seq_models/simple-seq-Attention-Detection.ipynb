{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the upper part is the defination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "Num GPUs Available:  0\n",
      "data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "#import tensorflow_addons as tfa\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "#%%\n",
    "\n",
    "folder = os.path.join('..','..','short_data_4_model','50')\n",
    "\n",
    "\n",
    "data_train = np.load(os.path.join(folder, 'train_data.npz'),'r')\n",
    "data_train = data_train['arr_0']\n",
    "\n",
    "data_valid = np.load(os.path.join(folder, 'valid_data.npz'),'r')\n",
    "data_valid = data_valid['arr_0']\n",
    "\n",
    "print('data loaded successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 60\n",
    "seq_len =50\n",
    "units = 200\n",
    "# no use\n",
    "embedding_dim = 256\n",
    "\n",
    "num_examples = len(data_train)\n",
    "steps_per_epoch = num_examples//BATCH_SIZE\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(data_train).shuffle(num_examples)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer encoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "embedding output shape: (batch size, sequence length, units) (60, 20, 3)\n",
      "Encoder output shape: (batch size, sequence length, units) (60, 20, 200)\n",
      "Encoder Hidden state shape: (batch size, units) (60, 200)\n"
     ]
    }
   ],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, encoding_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoding_units = encoding_units\n",
    "        self.inputs = tf.keras.layers.Input((seq_len, 3))\n",
    "\n",
    "        self.gru = keras.layers.GRU(self.encoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        inputs = x\n",
    "\n",
    "        output, h = self.gru(inputs, initial_state = hidden)\n",
    "        return inputs,output, h\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_size,self.encoding_units))\n",
    "    \n",
    "\n",
    "example_input_batch = np.zeros((BATCH_SIZE,20,3))\n",
    "# sample input\n",
    "encoder = Encoder( embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "embeded, sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "\n",
    "print('embedding output shape: (batch size, sequence length, units) {}'.format(embeded.shape))\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention result shape: (batch size, units) (60, 200)\n",
      "Attention weights shape: (batch_size, sequence_length, 1) (60, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query: encoder hidden state  (batch size, units) \n",
    "        # values: encoder output  (batch size, sequence length, units)\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "attention_layer = BahdanauAttention(units)\n",
    "\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (60, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, decoding_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoding_units = decoding_units\n",
    "        #self.embedding = keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = keras.layers.GRU(self.decoding_units,\n",
    "                                    return_sequences=True,\n",
    "                                    return_state=True,\n",
    "                                    recurrent_initializer='glorot_uniform')\n",
    "        self.fc = keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.fcn_4 = keras.layers.Dense(64, activation = 'relu')\n",
    "        self.fcn_5 = keras.layers.Dense(32, activation='relu')\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.decoding_units)\n",
    "\n",
    "    def call(self,  hidden, encoding_output,embedder):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoding_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        #x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), embedder], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        x = self.fc(output)\n",
    "\n",
    "\n",
    "        return x, state, attention_weights\n",
    "    \n",
    "\n",
    "vocab_size=3\n",
    "decoder = Decoder( vocab_size ,embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "embeded = tf.expand_dims(embeded[:,0,:],1)\n",
    "sample_decoder_output, _, _ = decoder(sample_hidden, sample_output, embeded)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50full_seq2seq_Attenetion_200_60\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.MeanSquaredError( reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "@tf.function\n",
    "def loss_function(real, pred):\n",
    "    \n",
    "    #mask = tf.math.logical_not(tf.math.equal(real, [0,0,0]))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    #mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    #loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "#checkpoint_dir = str(seq_len)+'test_seq2seq_Attenetion_'+str(units)+'_'+str(BATCH_SIZE)\n",
    "checkpoint_dir = str(seq_len)+'full_seq2seq_Attenetion_'+str(units)+'_'+str(BATCH_SIZE)\n",
    "\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.mkdir(checkpoint_dir)\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "\n",
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plyfile import PlyData\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from dipy.tracking.streamline import Streamlines\n",
    "from dipy.tracking.streamlinespeed import length\n",
    "                                          \n",
    "\n",
    "class PlyStruct:\n",
    "    \"\"\"Class that process poly data\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.ply_data = None\n",
    "        self.idx = None\n",
    "        self.properties = None\n",
    "        self.stream_line = None\n",
    "\n",
    "    def load_poly_data(self, subpath: str, num_prop: int = None):\n",
    "        \"\"\"\n",
    "        Load ply file\n",
    "        :param str subpath: path of ply file\n",
    "        :param int num_prop: optional parmeter to only take the first num properties (e.g. only x,y,z coordinates)\n",
    "        \"\"\"\n",
    "        ply = PlyData.read(subpath)\n",
    "        property_list = [ply['vertices'].data[name] for name in ply['vertices'].data.dtype.names]\n",
    "        self.ply_data = np.array(property_list).T\n",
    "        if num_prop:\n",
    "            self.ply_data = self.ply_data[:, :num_prop]\n",
    "        self.idx = ply['fiber'].data['endindex']\n",
    "        self.properties = ply['vertices'].data.dtype.names\n",
    "        if num_prop:\n",
    "            self.properties = self.properties[:num_prop]\n",
    "   \n",
    "\n",
    "    def gen_stream_line(self):\n",
    "        self.stream_line = []\n",
    "        self.stream_line.append(self.ply_data[0:self.idx[0],:])\n",
    "        for i in range(len(self.idx)-1):\n",
    "            self.stream_line.append(self.ply_data[self.idx[i]:self.idx[i+1],:])\n",
    "\n",
    "\n",
    "def zero_remove(darray):\n",
    "    \n",
    "    for i in range(np.shape(darray)[0]-1,-1,-1):\n",
    "        if not (np.around(darray[i], decimals=0) == 0 ).all():\n",
    "            return darray[0:i+1] \n",
    "            break\n",
    "\n",
    "\n",
    "def get_length(stream):\n",
    "    # to get the length of streams in this subject\n",
    "    len_table = []\n",
    "    for i in range(len(stream)):\n",
    "        len_table.append(np.shape(stream[i])[0])\n",
    "    return len_table\n",
    "\n",
    "def cal_mse(data_raw,data_pred):\n",
    "    mse = np.zeros(np.shape(data_raw)[0])\n",
    "    for i in range(np.shape(data_raw)[0]):\n",
    "        result = mean_squared_error(data_raw[i], data_pred[i])\n",
    "        mse[i] = result\n",
    "    return mse  \n",
    "\n",
    "\n",
    "def ply2np(name):\n",
    "    # convert a ply format to the matrix we are using\n",
    "    # input : '128127_ex_cc-body_shore.ply', name of a oly file\n",
    "    # optput: a matrix (#_of_fibers, #_of_vertex, 3 )\n",
    "    temo = PlyStruct()\n",
    "    temo.load_poly_data(os.path.join('..','data',name),num_prop=3)\n",
    "    #data = temo.ply_data\n",
    "  \n",
    "    temo.gen_stream_line()\n",
    "    stream = temo.stream_line\n",
    "\n",
    "\n",
    "    stream = np.array(stream)\n",
    "    len_table = get_length(stream)\n",
    "    subject = np.zeros((len(stream),np.max(len_table),3))\n",
    "    for i in range(len(stream)):\n",
    "        length = len_table[i]\n",
    "        subject[i,0:length,:] = stream[i]\n",
    "        \n",
    "    return subject\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def cal_mse(data_raw,data_pred):\n",
    "    mse = np.zeros(np.shape(data_raw)[0])\n",
    "    for i in range(np.shape(data_raw)[0]):\n",
    "        result = mean_squared_error(data_raw[i], data_pred[i])\n",
    "        mse[i] = result\n",
    "    return mse  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x16c6a643448>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(inputs):\n",
    "    result = np.zeros((np.shape(inputs)))\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "  \n",
    "    inference_batch_size = inputs.shape[0]\n",
    "    hidden = [tf.zeros((inference_batch_size, units))]\n",
    "    embeded_or,encoding_out, encoding_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    decoding_hidden = encoding_hidden\n",
    "\n",
    "\n",
    "\n",
    "    for t in range(seq_len):\n",
    "        embeded = tf.expand_dims(embeded_or[:,t,:],1)\n",
    "        predictions, decoding_hidden, attention_weights = decoder(decoding_hidden, encoding_out, embeded)\n",
    "\n",
    "    \n",
    "        result[:,t,:] = tf.squeeze(predictions)\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        decoding_input = predictions\n",
    "\n",
    "    return result, inputs\n",
    "\n",
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    plt.show()\n",
    "'''\n",
    "def make_pre(input):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
    "'''\n",
    "\n",
    "optimizer = keras.optimizers.Adam()\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A way to calculate the validate error\n",
    "'''\n",
    "for i in range(20):\n",
    "    f = open(os.path.join(checkpoint_dir,'checkpoint'), 'w')\n",
    "    f.write('model_checkpoint_path: \"ckpt-'+str(i+1)+'\"\\n')\n",
    "    f.write('all_model_checkpoint_paths: \"ckpt-'+str(i+1)+'\"\\n')\n",
    "    f.close()\n",
    "    checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "    \n",
    "    result, _ = evaluate(data_valid)\n",
    "    mse_idv =loss_function(data_valid, result)\n",
    "    print(str(i+1)+': '+str(mse_idv.numpy()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# type in the parameters\n",
    "name : the file name\\\n",
    "len_thre: length control value, length below it will be considered abnomal fiber\\\n",
    "thre_con: control value of threshold percentile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = '156031_ex_cc-body_shore.ply'\n",
    "len_thre = 40\n",
    "thre_con = 85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = ply2np(name)\n",
    "\n",
    "from dipy.tracking.streamlinespeed import (compress_streamlines, length,\n",
    "                                           set_number_of_points)\n",
    "streamlines_evl = Streamlines()\n",
    "for j in range(np.shape(bundle)[0]):\n",
    "    tmp = bundle[j]\n",
    "    tmp = zero_remove(tmp)\n",
    "    streamlines_evl.append(tmp)\n",
    "\n",
    "\n",
    "subsamp_sls = set_number_of_points(streamlines_evl, seq_len)\n",
    "\n",
    "data_test= np.array(subsamp_sls)\n",
    "result, _ = evaluate(data_test)\n",
    "mse_idv = cal_mse(data_test, result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "thre = np.percentile(mse_idv,thre_con)\n",
    "pred_1 = 1*(mse_idv<thre)\n",
    "\n",
    "\n",
    "bundle_str = Streamlines()\n",
    "\n",
    "for i in range(np.shape(bundle)[0]):\n",
    "    tmp = bundle[i]\n",
    "    tmp = zero_remove(tmp)\n",
    "    #tmp = tmp[~np.all(tmp == 0, axis=-1)]\n",
    "    #tmp = np.around(tmp, decimals=0)\n",
    "    bundle_str.append(tmp)\n",
    "    \n",
    "lengths = length(bundle_str)\n",
    "\n",
    "pred_2 = 1*(lengths > len_thre)\n",
    "\n",
    "pred = 1*((pred_1 + pred_2)>1)\n",
    "\n",
    "op_path = 'result'\n",
    "if not os.path.exists(op_path):\n",
    "    os.mkdir(op_path)\n",
    "\n",
    "save_path = os.path.join(op_path,'Detection'+name.split('_')[0]+'_Attention')\n",
    "np.save(save_path,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
